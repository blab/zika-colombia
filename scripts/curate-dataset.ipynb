{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Dataset curation: making the zika-colombia fasta files for main build and supplemental analyses\n",
    "  \n",
    "This notebook contains code needed to go from a raw download of all Zika genomes in `nextstrain/fauna` to the input fasta file for the zika-colombia specific analyses (which are custom `nextstrain/augur` builds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First thing that I am going to do is curate the dataset for the primary analysis. Curations steps that I would like to perform include:\n",
    "\n",
    "1. Removing sequences from any geographic areas that I do not want included in the analysis (e.g. Singapore)\n",
    "\n",
    "2. Removing any sequences that were up on GenBank, but were not actually published on, and for which we didn't receive the author's permission to include in our analysis.\n",
    "\n",
    "3. Removing repeat sequences of the same strains that were sequenced at different cell culture passage counts.\n",
    "\n",
    "4. Removing any sequences in which there are many sites with N's (i.e. very little of the genome sequence has informative base calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write any functions that I want to have for this work.\n",
    "\n",
    "## replace n's with gaps, and count n's in sequences (alignment checks)\n",
    "def count_n(sequence):\n",
    "    \"counts numbers of N's in a sequence to perform QC and see how many non-informative sites exist\"\n",
    "    counter = 0\n",
    "    for base in sequence:\n",
    "        if base == 'n':\n",
    "            counter +=1\n",
    "    return counter\n",
    "\n",
    "def sample_fasta_dict_without_replacement(dictionary,n_samples_to_draw):\n",
    "    \"\"\"randomly samples a Seq IO dictionary without replacement.\"\"\"\n",
    "    samples = random.sample(dictionary.items(), k=n_samples_to_draw)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Removing sequences from geographic areas outside the Americas and Oceania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 694 sequences downloaded from Fauna.\n",
      "Genomes from the following regions will be excluded: southeast_asia, japan_korea, china, and europe.\n",
      "There are 504 sequences meet the geographic criteria.\n"
     ]
    }
   ],
   "source": [
    "# Paths to files, keeping relational so that paths should work if someone downloads the repo as is.\n",
    "fauna_seqs_dict = SeqIO.to_dict(SeqIO.parse('../data/zika-fauna-2018-09-06.fasta', 'fasta'))\n",
    "print('There are {} sequences downloaded from Fauna.'.format(len(fauna_seqs_dict)))\n",
    "\n",
    "#Geographic pruning\n",
    "regions_to_exclude = ['southeast_asia', 'japan_korea', 'china', 'europe', 'africa']\n",
    "print('Genomes from the following regions will be excluded: {0}, {1}, {2}, and {3}.'.format(regions_to_exclude[0],regions_to_exclude[1],regions_to_exclude[2],regions_to_exclude[3]))\n",
    "\n",
    "geoPruned_seqs_dict = {fauna_seqs_dict[key].description:fauna_seqs_dict[key].seq for key in fauna_seqs_dict.keys() if key.split('|')[4] not in regions_to_exclude}\n",
    "print('There are {} sequences meet the geographic criteria.'.format(len(geoPruned_seqs_dict)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specifying which published and permission-given sequences we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 431 genomes that we can include in published analyses.\n"
     ]
    }
   ],
   "source": [
    "# read in the dataframe that has the permissions information,\n",
    "# then parse that to select out all strains that can be included in a publishable analysis\n",
    "# these are the strains that should be used, and form the fauna subset we want.\n",
    "\n",
    "genome_permissions = pd.read_csv('../data/genome-permissions-2018-09-06.txt', delimiter ='\\t')\n",
    "\n",
    "publishable_strains = []\n",
    "for i in range(len(genome_permissions)):\n",
    "    record = genome_permissions.iloc[i]\n",
    "    if record['permission_to_use'] != 'permission_not_received' and record['preliminarily_include'] == 'yes':\n",
    "        publishable_strains.append(record['strain_name'])\n",
    "\n",
    "print(\"There are {} genomes that we can include in published analyses.\".format(len(publishable_strains)))\n",
    "\n",
    "# using the strains in the publishable_strains list, pull out the full fauna headers (and sequences)\n",
    "# for each strain that can be published on.\n",
    "# also making sure we are only dealing with sequences within the countries of interest (Americans and Oceania)\n",
    "publishable_seqs_dict = {}\n",
    "for strain in publishable_strains:\n",
    "    for key in geoPruned_seqs_dict.keys():\n",
    "        if key.startswith(strain):\n",
    "            publishable_seqs_dict[key] = geoPruned_seqs_dict[key]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Specifying which Colombian sequences are cell culture passages of the same clinical sample and should NOT be included in the build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 33 sequences from different culture passages of COL/FLR/2015 that should be removed.\n",
      "There are now 398 sequences that should be included in the build.\n"
     ]
    }
   ],
   "source": [
    "col_flr_strains_to_remove = [strain for strain in publishable_strains if strain.startswith(\"COL/FLR\") and strain != 'COL/FLR/2015']\n",
    "print(\"There were {} sequences from different culture passages of COL/FLR/2015 that should be removed.\".format(len(col_flr_strains_to_remove)))\n",
    "\n",
    "publishable_seqs_no_cell_culture_passage_dict = {}\n",
    "for key in publishable_seqs_dict.keys():\n",
    "    if key.split('|')[0] not in col_flr_strains_to_remove:\n",
    "        publishable_seqs_no_cell_culture_passage_dict[key] = publishable_seqs_dict[key]\n",
    "\n",
    "assert(len(publishable_seqs_no_cell_culture_passage_dict.keys()) == (len(publishable_seqs_dict.keys()) -(len(col_flr_strains_to_remove))))\n",
    "print(\"There are now {} sequences that should be included in the build.\".format(len(publishable_seqs_no_cell_culture_passage_dict)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Specifying a quality theshold: Samples must have at least 50% of sites be unambiguous base calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At this point we have 385 sequences that meet quality specifications, are not cell culture passages, and are publishable.\n"
     ]
    }
   ],
   "source": [
    "n_counts_dict = {}\n",
    "for key in publishable_seqs_no_cell_culture_passage_dict.keys():\n",
    "    n_count = count_n(publishable_seqs_no_cell_culture_passage_dict[key])\n",
    "    n_counts_dict[key] = n_count\n",
    "            \n",
    "medium_qual_seqs = {}\n",
    "for key in n_counts_dict.keys():\n",
    "    if key.split('|')[5] == 'colombia':\n",
    "        medium_qual_seqs[key] = publishable_seqs_no_cell_culture_passage_dict[key]\n",
    "    else:\n",
    "        if float(n_counts_dict[key])/10769 < 0.5:\n",
    "            medium_qual_seqs[key] = publishable_seqs_no_cell_culture_passage_dict[key]\n",
    "\n",
    "print(\"At this point we have {} sequences that meet quality specifications, are not cell culture passages, and are publishable.\".format(len(medium_qual_seqs.keys())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Removing sequences that deviate significantly from the molecular clock.\n",
    "\n",
    "I found these sequences by running iterative Augur builds until no sequences got trimmed from the build for deviation. In this build, sequences whose rate of evolution is more than 4 interquartile distances outside of the clock rate across the dataset are trimmed.\n",
    "\n",
    "I've added this step here for reproducibility. I want to ensure that the proper input file can be made by running this notebook. So even though I did lots of builds to figure out what sequences should be removed, this list should be complete and result in the correct input file now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 374 sequences that are ready to go into our zika-colombia augur build.\n"
     ]
    }
   ],
   "source": [
    "clock_deviant_strains = ['USVI/28/2016', 'mex39/Mexico/2016', 'USA/2016/FL019', 'BRA/2016/FC_DQ12D1', 'DOM/2016/MA_WGS16_013', \n",
    "                         'Bahia02', 'Bahia03', 'GTM/2016/Guatemala_2411', 'MEX/2016/mex01', 'MEX/2016/mex37', 'NIC/2016/Nicaragua_3358']\n",
    "\n",
    "sequences_for_proper_build = {key:value for key,value in medium_qual_seqs.items() if key.split('|')[0] not in clock_deviant_strains}\n",
    "\n",
    "print(\"We now have {} sequences that are ready to go into our zika-colombia augur build.\".format(len(sequences_for_proper_build)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up some strain names...\n",
    "\n",
    "When I was looking at the input file I saw a couple of strain name errors or things that were confusing that I initially edited by hand. However, in the hopes of making this completely reproducible, I'm putting in some code below to make those edits. \n",
    "\n",
    "The issues were that 1) there were a couple of Colombian strains where the sampling date was in 2016, but the strain name said 2015, and 2) the Ecuadorian strain names were really hard to interpret as being from Ecuador, so I wanted to make those strain names more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_name_conversion_dict = {'COL/FH19/2015':'COL/FH19/2016',\n",
    "                              'EcEs062_16':'Ecuador/EcEs062_16/2016',\n",
    "                              'EcEs089_16':'Ecuador/EcEs089_16/2016',\n",
    "                              'COL/FH16/2015':'COL/FH16/2016'}\n",
    "\n",
    "fixed_names_sequences_for_proper_build = {}\n",
    "for key in sequences_for_proper_build.keys():\n",
    "    split_name = key.split('|')\n",
    "    if split_name[0] in strain_name_conversion_dict.keys():\n",
    "        new_name = strain_name_conversion_dict[split_name[0]] +'|'+ '|'.join(split_name[1:])\n",
    "        fixed_names_sequences_for_proper_build[new_name] = sequences_for_proper_build[key]\n",
    "    else:\n",
    "        fixed_names_sequences_for_proper_build[key] = sequences_for_proper_build[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N masking for site 10375 in the augur alignment\n",
    "\n",
    "Nanopolish seems to have some issue where at this site specifically it calls the reference rather than a SNP (despite strong evidence in the BAM stack for a SNP, and the fact that all other sequences in the Americas differ from French Polynesian sequences at this site). Since this seems to artifactual, I've N-masked this site in all sequences sequenced on Nanopore (since these are the sequences that are nanopolished)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sequence_with_miscall = 'CTTAGTGTTG'\n",
    "n_masked_sequence_replacement ='CTTANTGTTG'\n",
    "\n",
    "properly_n_masked_sequences_for_proper_build = {}\n",
    "for key in fixed_names_sequences_for_proper_build.keys():\n",
    "    if key.split('|')[4] == 'oceania': #a G at this site is correct for oceania sequences!\n",
    "        properly_n_masked_sequences_for_proper_build[key] = fixed_names_sequences_for_proper_build[key]\n",
    "    elif target_sequence_with_miscall in fixed_names_sequences_for_proper_build[key]:\n",
    "        masked_seq = fixed_names_sequences_for_proper_build[key].replace(target_sequence_with_miscall,n_masked_sequence_replacement)\n",
    "        properly_n_masked_sequences_for_proper_build[key] = masked_seq\n",
    "    else:\n",
    "        properly_n_masked_sequences_for_proper_build[key] = fixed_names_sequences_for_proper_build[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, let's write these sequences out to a fasta file to run in our augur build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write sequences to file\n",
    "with open('../data/input.fasta','w') as file:\n",
    "    for key in properly_n_masked_sequences_for_proper_build.keys():\n",
    "        file.write(str('>' + key + '\\n' + properly_n_masked_sequences_for_proper_build[key] + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make input data files for rarefaction curve supplemental analysis\n",
    "\n",
    "Next up, I want to make a little additional dataset for a supplemental analysis I'm doing, making rarefaction curves to investigate how many introductions one observes given the numbers of sequences sampled. I'm going to do this analysis for sequences from Colombia and sequences from Mexico, and it involves subsampling them down as well. \n",
    "\n",
    "In addition to this, I will need to make a \"background sequences\" file. This fasta will contain all of the sequences from the Americas that are used in the main analysis build EXCEPT for the country for which the subsampling is occurring (e.g. background file for Mexican subsampling analysis will contain all other American sequences in the main build, including all Colombian, but won't have any Mexican seuquences). Later on the in the build subsampled fastas will be concatenated with background sequences fasta in order to make Augur build input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_main_build_sequences = SeqIO.to_dict(SeqIO.parse('../data/publishable-zika-fauna-2018-10-15.fasta', 'fasta'))\n",
    "background_seqs_no_mexico = {all_main_build_sequences[key].description:all_main_build_sequences[key].seq for key in all_main_build_sequences.keys() if key.split('|')[5] != 'mexico'}\n",
    "background_seqs_no_colombia = {all_main_build_sequences[key].description:all_main_build_sequences[key].seq for key in all_main_build_sequences.keys() if key.split('|')[5]!= 'colombia'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 745 sequences downloaded from Fauna in the rarefaction import.\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "# Making a dictionary that has the Mexican sequences I'd like to use in the rarefaction analysis.\n",
    "# Needs to be current, publishable, high-quality sequences. Some of these are newer than the original zika-colombia build\n",
    "# because data lock happened before these sequences were published, so I'm actually pulling Mexican sequences for this analysis\n",
    "# from a more recent fauna download.\n",
    "\n",
    "rarefaction_seqs_dict = SeqIO.to_dict(SeqIO.parse('../data/zika.fasta', 'fasta'))\n",
    "print ('There are {} sequences downloaded from Fauna in the rarefaction import.'.format(len(rarefaction_seqs_dict)))\n",
    "\n",
    "mexico_seqs_dict = {}\n",
    "for key in rarefaction_seqs_dict.keys():\n",
    "    if key.split('|')[5] == 'mexico':\n",
    "        mexico_seqs_dict[key] = rarefaction_seqs_dict[key]\n",
    "    \n",
    "#notably not all of these sequences can be published on, so the following author's sequences should be dropped from the analysis\n",
    "drop_authors = ['Sevilla-Reyes','Balaraman','Izquierdo', 'Valdespino-Vazquez']\n",
    "mexican_seqs_for_use = {key:mexico_seqs_dict[key] for key in mexico_seqs_dict.keys() if key.split('|')[10] not in drop_authors}\n",
    "\n",
    "#I also don't want to include sequences that are 50% N, so checking quality. \n",
    "# I'm going to say that the sequences need to be high quality: they need to have at least 80% informative bases.\n",
    "samples_to_exclude_due_to_quality = []\n",
    "for key in mexican_seqs_for_use.keys():\n",
    "    n_count = count_n(mexican_seqs_for_use[key])\n",
    "    if n_count > (10769*0.2):\n",
    "        samples_to_exclude_due_to_quality.append(key)\n",
    "        \n",
    "high_qual_mexican_seqs_for_use = {key:mexican_seqs_for_use[key] for key in mexican_seqs_for_use.keys() if key not in samples_to_exclude_due_to_quality}\n",
    "print(len(high_qual_mexican_seqs_for_use))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike the Mexican sequences that needed to be pulled from a newer Fauna download, there are no new Colombian sequences\n",
    "# that have come out since the data lock for the manuscript. Additionally all of the Colombian sequences in the main zika-colombia \n",
    "# build have already been filtered for quality etc. These are the sequences that I want to down sample for my rarefaction curve analysis\n",
    "# Because I want the sequences included in the build, I'm going to integrate the input fasta sequences and the dropped_sequences files,\n",
    "# to ensure I only keep the colombian sequences that shouldn't be dropped.\n",
    "\n",
    "zika_col_seqs = SeqIO.to_dict(SeqIO.parse(\"../data/publishable-zika-fauna-2018-10-15.fasta\",\"fasta\"))\n",
    "dropped_strains = [line.strip() for line in open(\"../config/dropped_strains.txt\",\"r\")]    \n",
    "colombian_seq_dict = {key:value for key,value in zika_col_seqs.items() if key.split('|')[5] == 'colombia' and key.split('|')[0] not in dropped_strains }\n",
    "assert len(colombian_seq_dict.keys()) == 20, \"Number of sequences here does not match number of sequences in live build.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm subsampling down both my Colombian and my Mexican sequence dictionaries to make the input datasets for the rarefaction curve analysis, which looks at how many introductions into a country (either Mexico or Colombia) are observed given x numbers of sequences sampled from that country. The idea is to see when this relationship asymptotes, i.e. how many sequences do you need to observe most of the introductions to a country that occurred.\n",
    "\n",
    "To get the data sets for this analysis, I need to subsample down one countries sequences, and re-run the augur pipelines with the rest of the build the same, and just look at how introductions to that country changed given numbers of sequences from that country obtained. I'll do this both for Colombia and for Mexico, but separately (i.e. any subsampled Mexican sequences will be run with all Colombian sequences and vice versa).\n",
    "\n",
    "The subsampling scheme is as follows. Try 1 sequence, 2 sequences, 3 sequences ... all the sequences, and look at numbers of introductions observed. For each subsample amount (e.g. 4 sequences) there will be 5 trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123456) # set seed for reproducibility\n",
    "\n",
    "#let's make the directory structure that needs to exist to house our files for AWS batching craziness.\n",
    "replicate_trial_map = {1:[1,2,3,4,5], 2:[6,7,8,9,10], 3:[11,12,13,14,15], 4:[16,17,18,19,20], 5:[21,22,23,24,25], 6:[26,27,28,29,30]}\n",
    "\n",
    "#MAKE FILES FOR MEXICO\n",
    "for key in replicate_trial_map.keys():\n",
    "    replicate_number = key\n",
    "    os.makedirs(\"../supplemental-analysis/rarefaction-curves/mexico/mex-replicate-{}/data/\".format(replicate_number))\n",
    "    for i in range(1, len(high_qual_mexican_seqs_for_use)+1):\n",
    "        n_seqs = i\n",
    "        subsample = sample_fasta_dict_without_replacement(high_qual_mexican_seqs_for_use,n_seqs)\n",
    "        for trial_number in replicate_trial_map[key]:\n",
    "            with open(\"../supplemental-analysis/rarefaction-curves/mexico/mex-replicate-{}/data/mex_{}_seqs_trial_{}.fasta\".format(replicate_number, n_seqs, trial_number),'w') as file:\n",
    "                for sequence in subsample:\n",
    "                    file.write(str(\">\"+sequence[0] + '\\n' + sequence[1].seq + '\\n'))\n",
    "                    \n",
    "#MAKE FILES FOR COLOMBIA                   \n",
    "for key in replicate_trial_map.keys():\n",
    "    replicate_number = key\n",
    "    os.makedirs(\"../supplemental-analysis/rarefaction-curves/colombia/col-replicate-{}/data/\".format(replicate_number))\n",
    "    for i in range(1, len(colombian_seq_dict)+1):\n",
    "        n_seqs = i\n",
    "        subsample = sample_fasta_dict_without_replacement(colombian_seq_dict,n_seqs)\n",
    "        for trial_number in replicate_trial_map[key]:\n",
    "            with open(\"../supplemental-analysis/rarefaction-curves/colombia/col-replicate-{}/data/colombia_{}_seqs_trial_{}.fasta\".format(replicate_number, n_seqs, trial_number),'w') as file:\n",
    "                for sequence in subsample:\n",
    "                    file.write(str(\">\"+sequence[0] + '\\n' + sequence[1].seq + '\\n'))\n",
    "                                 \n",
    "                    \n",
    "#need to have a background sequences file in every replicate directory\n",
    "#Mexico\n",
    "for key in replicate_trial_map.keys():\n",
    "    with open(\"../supplemental-analysis/rarefaction-curves/mexico/mex-replicate-{}/data/background_seqs_no_mexico.fasta\".format(key),\"w\") as file:\n",
    "        for key in background_seqs_no_mexico.keys():\n",
    "            file.write(str(\">\" + key + \"\\n\" + background_seqs_no_mexico[key] + \"\\n\" ))\n",
    "#Colombia        \n",
    "for key in replicate_trial_map.keys():\n",
    "    with open(\"../supplemental-analysis/rarefaction-curves/colombia/col-replicate-{}/data/background_seqs_no_colombia.fasta\".format(key),\"w\") as file:\n",
    "        for key in background_seqs_no_colombia.keys():\n",
    "            file.write(str(\">\"+ key + \"\\n\" + background_seqs_no_colombia[key] + \"\\n\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
