{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Dataset curation: making the zika-colombia fasta files for main build and supplemental analyses\n",
    "  \n",
    "This notebook contains code needed to go from a raw download of all Zika genomes in `nextstrain/fauna` to the input fasta file for the zika-colombia specific analyses (which are custom `nextstrain/augur` builds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that I am going to do is curate the dataset for the primary analysis. Curations steps that I would like to perform include:\n",
    "\n",
    "1. Removing sequences from any geographic areas that I do not want included in the analysis (e.g. Singapore)\n",
    "\n",
    "2. Removing any sequences that were up on GenBank, but were not actually published on, and for which we didn't receive the author's permission to include in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 694 sequences downloaded from Fauna.\n",
      "Genomes from the following regions will be excluded: southeast_asia, japan_korea, china, and europe.\n",
      "There are 504 sequences meet the geographic criteria.\n"
     ]
    }
   ],
   "source": [
    "# Paths to files, keeping relational so that paths should work if someone downloads the repo as is.\n",
    "fauna_seqs_dict = SeqIO.to_dict(SeqIO.parse('../data/zika-fauna-2018-09-06.fasta', 'fasta'))\n",
    "print 'There are {} sequences downloaded from Fauna.'.format(len(fauna_seqs_dict))\n",
    "\n",
    "#Geographic pruning\n",
    "regions_to_exclude = ['southeast_asia', 'japan_korea', 'china', 'europe', 'africa']\n",
    "print 'Genomes from the following regions will be excluded: {0}, {1}, {2}, and {3}.'.format(regions_to_exclude[0],regions_to_exclude[1],regions_to_exclude[2],regions_to_exclude[3])\n",
    "\n",
    "geoPruned_seqs_dict = {fauna_seqs_dict[key].description:fauna_seqs_dict[key].seq for key in fauna_seqs_dict.keys() if key.split('|')[4] not in regions_to_exclude}\n",
    "print 'There are {} sequences meet the geographic criteria.'.format(len(geoPruned_seqs_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#at this point, print out all the genomes are that are left, and ensure that permissions are available.\n",
    "accessions_to_check = [key for key in geoPruned_seqs_dict.keys()]\n",
    "\n",
    "strain_names = [accession.split('|')[0] for accession in accessions_to_check]\n",
    "ncbi_id = [accession.split('|')[2] for accession in accessions_to_check]\n",
    "lead_author =  [accession.split('|')[10].replace('et al','') for accession in accessions_to_check]\n",
    "\n",
    "\n",
    "accessions_df = pd.DataFrame(np.column_stack([strain_names, ncbi_id, lead_author]), columns= ['strain_name', 'accession_number','lead_author'])\n",
    "accessions_df.to_csv('../data/all_included_accessions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 431 genomes that we can include in published analyses.\n"
     ]
    }
   ],
   "source": [
    "# read in the dataframe that has the permissions information,\n",
    "# then parse that to select out all strains that can be included in a publishable analysis\n",
    "# these are the strains that should be used, and form the fauna subset we want.\n",
    "\n",
    "genome_permissions = pd.read_csv('../data/genome-permissions-2018-09-06.txt', delimiter ='\\t')\n",
    "\n",
    "publishable_strains = []\n",
    "for i in range(len(genome_permissions)):\n",
    "    record = genome_permissions.iloc[i]\n",
    "    if record['permission_to_use'] != 'permission_not_received' and record['preliminarily_include'] == 'yes':\n",
    "        publishable_strains.append(record['strain_name'])\n",
    "\n",
    "print \"There are {} genomes that we can include in published analyses.\".format(len(publishable_strains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the strains in the publishable_strains list, pull out the full fauna headers (and sequences)\n",
    "# for each strain that can be published on.\n",
    "# then make a new fauna-formatted fasta file that can be read in to Augur for analysis.\n",
    "\n",
    "publishable_seqs_dict = {}\n",
    "for strain in publishable_strains:\n",
    "    for key in geoPruned_seqs_dict.keys():\n",
    "        if key.startswith(strain):\n",
    "            publishable_seqs_dict[key] = geoPruned_seqs_dict[key]\n",
    "\n",
    "with open('../data/publishable-zika-fauna-2018-09-06.fasta','w') as file:\n",
    "    for key in publishable_seqs_dict.keys():\n",
    "        file.write(str('>' + key + '\\n' + publishable_seqs_dict[key] + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to check the quality of the sequences that I will use. Specifically, I want to know what proportion of the genome we have informative sequence calls for each sample. The following code blocks do things like count numbers of N's in a sequence, and separate out high quality sequences from low/medium quality sequences given different thresholds for the numbers of N's that are acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace n's with gaps, and count n's in sequences (alignment checks)\n",
    "\n",
    "def count_n(sequence):\n",
    "    counter = 0\n",
    "    for base in sequence:\n",
    "        if base == 'n':\n",
    "            counter +=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = SeqIO.to_dict(SeqIO.parse('../data/publishable-zika-fauna-2018-09-06.fasta', 'fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_counts_dict = {}\n",
    "for key in sequences.keys():\n",
    "    n_count = count_n(sequences[key].seq)\n",
    "    n_counts_dict[key] = n_count\n",
    "\n",
    "gaps_not_n_seqs = {}\n",
    "for key in sequences.keys():\n",
    "    n_seq = str(sequences[key].seq)\n",
    "    gap_seq = n_seq.replace('n','-')\n",
    "    gaps_not_n_seqs[key] = gap_seq\n",
    "    \n",
    "high_qual_seqs = {}\n",
    "for key in n_counts_dict.keys():\n",
    "    if key.split('|')[5] == 'colombia':\n",
    "        high_qual_seqs[key] = sequences[key].seq\n",
    "    else:\n",
    "        if float(n_counts_dict[key])/10769 < 0.2:\n",
    "            high_qual_seqs[key] = sequences[key].seq\n",
    "            \n",
    "medium_qual_seqs = {}\n",
    "for key in n_counts_dict.keys():\n",
    "    if key.split('|')[5] == 'colombia':\n",
    "        medium_qual_seqs[key] = sequences[key].seq\n",
    "    else:\n",
    "        if float(n_counts_dict[key])/10769 < 0.5:\n",
    "            medium_qual_seqs[key] = sequences[key].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n",
      "415\n"
     ]
    }
   ],
   "source": [
    "print len(high_qual_seqs)\n",
    "with open('../data/publishable-zika-fauna-2018-09-06-high-quality.fasta','w') as file:\n",
    "    for key in high_qual_seqs.keys():\n",
    "        file.write(str('>' + key + '\\n' + high_qual_seqs[key] + '\\n'))\n",
    "        \n",
    "print len(medium_qual_seqs)\n",
    "with open('../data/publishable-zika-fauna-2018-09-06-medium-quality.fasta','w') as file:\n",
    "    for key in medium_qual_seqs.keys():\n",
    "        file.write(str('>' + key + '\\n' + medium_qual_seqs[key] + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/publishable-zika-fauna-2018-09-06-gapped.fasta','w') as file:\n",
    "    for key in gaps_not_n_seqs.keys():\n",
    "        file.write(str('>' + key + '\\n' + gaps_not_n_seqs[key] + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, I want to make a little additional dataset for a supplemental analysis I'm doing, making rarefaction curves to investigate how many introductions one observes given the numbers of sequences sampled. I'm going to do this analysis for sequences from Colombia and sequences from Mexico, and it involves subsampling them down as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 745 sequences downloaded from Fauna in the rarefaction import.\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "rarefaction_seqs_dict = SeqIO.to_dict(SeqIO.parse('/Users/alliblk/Desktop/gitrepos/fauna/data/zika.fasta', 'fasta'))\n",
    "print ('There are {} sequences downloaded from Fauna in the rarefaction import.'.format(len(rarefaction_seqs_dict)))\n",
    "\n",
    "#mexico_seqs_dict = {key:rarefaction_seqs_dict[key] for key in rarefaction_seqs_dict.keys if key.split('|')[5] == 'mexico')}\n",
    "mexico_seqs_dict = {}\n",
    "for key in rarefaction_seqs_dict.keys():\n",
    "    if key.split('|')[5] == 'mexico':\n",
    "        mexico_seqs_dict[key] = rarefaction_seqs_dict[key]\n",
    "\n",
    "#for key in mexico_seqs_dict.keys():\n",
    "    #print(key.split('|')[10] + ' ' + key.split('|')[2]) \n",
    "\n",
    "    \n",
    "    #notably not all of these sequences can be published on, so the following author's sequences should be dropped from the analysis\n",
    "drop_authors = ['Sevilla-Reyes','Balaraman','Izquierdo', 'Valdespino-Vazquez']\n",
    "\n",
    "mexican_seqs_for_use = {key:mexico_seqs_dict[key] for key in mexico_seqs_dict.keys() if key.split('|')[10] not in drop_authors}\n",
    "\n",
    "#I also don't want to include sequences that are 50% N, so checking quality. \n",
    "# I'm going to say that the sequences need to be high quality: they need to have at least 80% informative bases.\n",
    "samples_to_exclude_due_to_quality = []\n",
    "for key in mexican_seqs_for_use.keys():\n",
    "    n_count = count_n(mexican_seqs_for_use[key])\n",
    "    if n_count > (10769*0.2):\n",
    "        samples_to_exclude_due_to_quality.append(key)\n",
    "        \n",
    "high_qual_mexican_seqs_for_use = {key:mexican_seqs_for_use[key] for key in mexican_seqs_for_use.keys() if key not in samples_to_exclude_due_to_quality}\n",
    "print(len(high_qual_mexican_seqs_for_use))\n",
    "\n",
    "#Okay, high_qual_mexican_seqs is the set that should be used/subsampled for rarefaction analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
